@article{Kreutz-Delgado2003,
abstract = {Algorithms for data-driven learning of domain-specific overcomplete dictionaries are developed to obtain maximum likelihood and maximum a posteriori dictionary estimates based on the use of Bayesian models with concave/Schur-concave (CSC) negative log priors. Such priors are appropriate for obtaining sparse representations of environmental signals within an appropriately chosen (environmentally matched) dictionary. The elements of the dictionary can be interpreted as concepts, features, or words capable of succinct expression of events encountered in the environment (the source of the measured signals). This is a generalization of vector quantization in that one is interested in a description involving a few dictionary entries (the proverbial "25 words or less"), but not necessarily as succinct as one entry. To learn an environmentally adapted dictionary capable of concise expression of signals generated by the environment, we develop algorithms that iterate between a representative set of sparse representations found by variants of FOCUSS and an update of the dictionary using these sparse representations. Experiments were performed using synthetic data and natural images. For complete dictionaries, we demonstrate that our algorithms have improved performance over other independent component analysis (ICA) methods, measured in terms of signal-to-noise ratios of separated sources. In the overcomplete case, we show that the true underlying dictionary and sparse sources can be accurately recovered. In tests with natural images, learned overcomplete dictionaries are shown to have higher coding efficiency than complete dictionaries; that is, images encoded with an overcomplete dictionary have both higher compression (fewer bits per pixel) and higher accuracy (lower mean square error).},
archivePrefix = {arXiv},
arxivId = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2944020/pdf/nihms234072.pdf},
author = {Kreutz-Delgado, Kenneth and Murray, Joseph F and Rao, Bhaskar D and Engan, Kjersti and Lee, Te-Won and Sejnowski, Terrence J},
doi = {10.1162/089976603762552951},
eprint = {/www.ncbi.nlm.nih.gov/pmc/articles/PMC2944020/pdf/nihms234072.pdf},
file = {:C$\backslash$:/Users/Reich/Documents/Mendeley Desktop/Neural computation/2003\_Dictionary learning algorithms for sparse representation.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural computation},
number = {2},
pages = {349--396},
pmid = {12590811},
primaryClass = {http:},
title = {{Dictionary learning algorithms for sparse representation.}},
volume = {15},
year = {2003}
}
@article{Lu2013,
abstract = {Online dictionary learning is particularly useful for processing large-scale and dynamic data in computer vision. It, however, faces the major difficulty to incorporate robust functions, rather than the square data fitting term, to handle outliers in training data. In this paper, we propose a new online framework enabling the use of \&\#x2113;<sup>1</sup> sparse data fitting term in robust dictionary learning, notably enhancing the usability and practicality of this important technique. Extensive experiments have been carried out to validate our new framework.},
author = {Lu, Cewu and Shi, Jiaping and Jia, Jiaya},
doi = {10.1109/CVPR.2013.60},
file = {:C$\backslash$:/Users/Reich/Documents/Mendeley Desktop/Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition/2013\_Online robust dictionary learning.pdf:pdf},
isbn = {978-0-7695-4989-7},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer  Vision and Pattern Recognition},
keywords = {Dictionary Learning,Online Learning,Robust Statistics},
pages = {415--422},
title = {{Online robust dictionary learning}},
year = {2013}
}
@article{Zhong2005,
abstract = { The spherical k-means algorithm, i.e., the k-means algorithm with cosine similarity, is a popular method for clustering high-dimensional text data. In this algorithm, each document as well as each cluster mean is represented as a high-dimensional unit-length vector. However, it has been mainly used in hatch mode. Thus is, each cluster mean vector is updated, only after all document vectors being assigned, as the (normalized) average of all the document vectors assigned to that cluster. This paper investigates an online version of the spherical k-means algorithm based on the well-known winner-take-all competitive learning. In this online algorithm, each cluster centroid is incrementally updated given a document. We demonstrate that the online spherical k-means algorithm can achieve significantly better clustering results than the batch version, especially when an annealing-type learning rate schedule is used. We also present heuristics to improve the speed, yet almost without loss of clustering quality.},
author = {Zhong, Shi},
doi = {10.1109/IJCNN.2005.1556436},
file = {:C$\backslash$:/Users/Reich/Documents/Mendeley Desktop/Proceedings of the International Joint Conference on Neural Networks/2005\_Efficient online spherical k-means clustering.pdf:pdf},
isbn = {0780390482},
journal = {Proceedings of the International Joint Conference on Neural Networks},
pages = {3180--3185},
title = {{Efficient online spherical k-means clustering}},
volume = {5},
year = {2005}
}
@article{Adlakha,
author = {Adlakha, Deepali},
file = {:C$\backslash$:/Users/Reich/Documents/Mendeley Desktop/Unknown/Unknown\_Dictionary Learning Algorithms and Applications.pdf:pdf},
title = {{Dictionary Learning Algorithms and Applications}}
}
@article{Nam2013,
author = {Nam, S. and Davies, M.E. and Elad, M. and Gribonval, R.},
doi = {10.1016/j.acha.2012.03.006},
file = {:C$\backslash$:/Users/Reich/Documents/Mendeley Desktop/Applied and Computational Harmonic Analysis/2013\_The cosparse analysis model and algorithms.pdf:pdf},
issn = {10635203},
journal = {Applied and Computational Harmonic Analysis},
month = jan,
number = {1},
pages = {30--56},
publisher = {Elsevier Inc.},
title = {{The cosparse analysis model and algorithms}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1063520312000450},
volume = {34},
year = {2013}
}
@article{Dai2008,
abstract = {We propose a new method for reconstruction of sparse signals with and without noisy perturbations, termed the subspace pursuit algorithm. The algorithm has two important characteristics: low computational complexity, comparable to that of orthogonal matching pursuit techniques, and reconstruc- tion accuracy of the same order as that of LP optimization methods. The presented analysis shows that in the noiseless setting, the proposed algorithm can exactly reconstruct arbitrary sparse signals provided that the sensing matrix satisfies the restricted isometry property with a constant parameter. In the noisy setting and in the case that the signal is not exactly sparse, it can be shown that the mean squared error of the reconstruction is upper bounded by constant multiples of the measurement and signal perturbation energies.},
archivePrefix = {arXiv},
arxivId = {arXiv:0803.0811v1},
author = {Dai, Wei and Milenkovic, Olgica},
doi = {10.1109/TIT.2009.2016006},
eprint = {arXiv:0803.0811v1},
file = {:C$\backslash$:/Users/Reich/Documents/Mendeley Desktop/IEEE Transactions on Infirmation Theroy/2008\_Subspace pursuit for compressive sensing Closing the gap between performance and complexity.pdf:pdf},
issn = {0018-9448},
journal = {IEEE Transactions on Infirmation Theroy},
pages = {2230--2249},
title = {{Subspace pursuit for compressive sensing: Closing the gap between performance and complexity}},
url = {http://oai.dtic.mil/oai/oai?verb=getRecord\&metadataPrefix=html\&identifier=ADA528357},
volume = {55},
year = {2008}
}
@inproceedings{Shu2014,
author = {Shu, Xianbiao and Yang, Jianchao and Ahuja, Narendra},
booktitle = {2014 IEEE International Conference on Computational Photography, ICCP 2014},
doi = {10.1109/ICCPHOT.2014.6831806},
file = {:C$\backslash$:/Users/Reich/Documents/Mendeley Desktop/2014 IEEE International Conference on Computational Photography, ICCP 2014/2014\_Non-local compressive sampling recovery.pdf:pdf},
isbn = {9781479951888},
title = {{Non-local compressive sampling recovery}},
year = {2014}
}
